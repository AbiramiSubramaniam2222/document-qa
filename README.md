ğŸ› ï¸ Badges 

![Python Version](https://img.shields.io/badge/Python-3.9%2B-blue)
![LangChain](https://img.shields.io/badge/LangChain-RAG-orange)
![FAISS](https://img.shields.io/badge/FAISS-Vector%20Search-green)
![License](https://img.shields.io/badge/License-MIT-lightgrey)



ğŸ“„ Document Question-Answering System
ğŸ¯ HR-Friendly Summary

This project is an end-to-end Document Q&A system that reads PDF documents, breaks them into meaningful chunks, stores them as vector embeddings using FAISS, and answers user questions using a Retrieval-Augmented Generation pipeline.
It demonstrates practical skills in LangChain, embeddings, vector search, LLM integration, and building modular AI pipelines â€” exactly the kind of work done in real industry RAG systems.

âš™ï¸ Tech Stack

Languages & Frameworks

Python 3.9+

LangChain

FAISS

OpenAI / LLM APIs

Libraries

PyPDF2 / pdfplumber (PDF extraction)

SentenceTransformers / Embeddings

Dotenv

YAML configs

Tools

VS Code

Virtual Environments (venv)

Git & GitHub

ğŸ“Œ Overview

This project implements an end-to-end document question-answering pipeline. It uses LangChain, vector embeddings, FAISS, and LLMs to answer questions based on the content of PDFs or text documents.

âš™ï¸ Features

Automatic PDF extraction and smart text chunking

FAISS-based vector embeddings for fast similarity search

Retrieval-Augmented Generation (RAG)

Clean and simple script-based interface

Modular design, easy to extend

ğŸš€ Installation
1. Clone the repository
git clone https://github.com/AbiramiSubramaniam2222/document-qa.git
cd document-qa

2. Create and activate a virtual environment
python3 -m venv venv
source venv/bin/activate

3. Install dependencies
pip install -r requirements.txt

ğŸ”’ Configuration

API keys stored in .env

max_size_tokens = 1000 (controls text passed to LLM)

Chunking, model configs adjustable via YAML

â–¶ï¸ Usage

Run directly inside VS Code terminal:

python3 document_qa_demo.py

How it works

Load PDF

Ask a question

Retrieve relevant chunks via FAISS

LLM generates final answer

âš ï¸ Limitations

Context window limited to 1000 tokens

Scanned PDFs need OCR

Chunking affects accuracy

ğŸ“ Folder Structure
document-qa/
â”œâ”€â”€ app.py                # Main application entry point (optional)
â”œâ”€â”€ document_qa_demo.py   # Script you run inside VS Code
â”œâ”€â”€ data/                 # Raw documents (local PDFs)
â”œâ”€â”€ embeddings/           # FAISS index files
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ README.md             # Project documentation
â”œâ”€â”€ screenshots/          # Added for project screenshots
â”œâ”€â”€ venv/                 # Virtual environment (ignored in Git)
â””â”€â”€ .gitignore            # Ignore rules


## ğŸ–¼ï¸ Screenshots  
The project includes one demo screenshot that shows the final answer generated by the LLM during retrieval-based question answering.

The image is placed inside the `screenshots/` folder.

![Demo Output](./screenshots/gpt_transformer_answer.png)

ğŸ§© Architecture Overview
PDF File
   â”‚ Extract text
   â–¼
Text Chunking (chunk + overlap)
   â”‚ Embed chunks
   â–¼
Embedding Model
   â”‚ Store vectors
   â–¼
FAISS Index
   â”‚ Retrieve top-K
   â–¼
Retrieved Context
   â”‚ Send to LLM
   â–¼
LLM Answer

ğŸ” RAG Workflow (Simple View)

User Question
â†’ Semantic Search (FAISS)
â†’ Retrieve Relevant Chunks
â†’ LLM + Context
â†’ Final Answer

ğŸ“ Example I/O

Input
PDF: Insurance Claim Policy Document
Question:

â€œIs accidental damage covered under this policy?â€

Output
Yes, accidental damage is covered. According to Section 2.1 of the policy, the plan includes coverage for accidental physical damage to the insured item, provided the claim is filed within 30 days of the incident.

ğŸ”® Future Enhancements

Here are some improvements planned for the next version of this project:

Web UI using Streamlit or FastAPI
A clean interface where users can upload documents and ask questions without using the terminal.

Multi-PDF support
Ability to load multiple documents into a single FAISS index for cross-document querying.

Larger context support
Implementing sliding-window retrieval and re-ranking to handle long documents beyond the 1000-token limit.

Advanced rerankers (BERT / Cross-Encoder)
More accurate retrieval by adding a reranking model on top of FAISS.

Metadata-based search
Page numbers, headings, and timestamps included in search to improve context grounding.

Support for scanned PDFs
Automatic OCR integration for image-based documents.

Caching layer
Store previous answers to reduce LLM cost and speed up repeated queries.

Docker containerization
Run the entire pipeline anywhere with one command.
